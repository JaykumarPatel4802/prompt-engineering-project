---
title: "Milestone 3"
subtitle: "Chatbot"
date: today
author: Jaykumar Patel, Laith Altarabishi, Harold Zhong
format:
  html:
    toc: true
    embed-resources: true
mainfont: TeX Gyre Schola
monofont: JetBrainsMono Nerd Font
mathfont: TeX Gyre Schola Math Regular
jupyter: python3
---

# Intro

This documents our creation of a chatbot.

# Data card information

## Dataset

The dataset we will use is `abisee/cnn_dailymail` which is available on Hugging Face.

At a high level, this dataset contains over 300k entries, containing CNN daily mail and their highlights.

## Source

The dataset was created as a part of the research paper [Abstractive Text Summarization using Sequence-to-sequence RNNs and
Beyond](https://arxiv.org/pdf/1602.06023v5). The authors of that paper created this dataset (`abisee/cnn_dailymail`) by modifying an existing corpus (created by authors of [Teaching Machines to Read and Comprehend](https://arxiv.org/pdf/1506.03340)) to include the summaries of articles.

The code for the original data collection can be found at <https://github.com/google-deepmind/rc-data>. The updated code that does not anonymize the data can be found at <https://github.com/abisee/cnn-dailymail>.

The text (articles) from which the dataset is collected is written by journalists at CNN and Daily Mail. There are articles from CNN, covering April 2007 to April 2015, and Daily Mail, from June 2010 to April 2015. These articles were collected via the Wayback Machine archives of their respective websites.

## URL

The URL of the dataset on Hugging Face is: <https://huggingface.co/datasets/abisee/cnn_dailymail>

## Repository

The repository that contains the `abisee/cnn_dailymail` dataset is Hugging Face.

## Task we intend to use it for

We plan to use this dataset for the task of summarization. Specifically, we aim to use this dataset to finetune a model to extract the important information, or "highlights", from a news article.

After finetuning, our ChatBot will be able to highlight the key points given a news article.

## Size

The following table shows the size specifications (Memory, Number of Rows, and Number of Columns) for each of the Splits (Train, Validation, and Test).

| Split | Memory (Bytes) | Number of Rows | Number of Columns |
|-------|----------------|----------------|-------------------|
| Train | 1101073761 | 287113 | 3 |
| Validation | 54494050 | 13368 | 3 |
| Test | 45671279 | 11490 | 3

The complete dataset has 312,971 rows.

## Structure

The following table shows the structure of the dataset.

| Dataset Split |	Number of Instances (Rows) in Split |
|---------------|------------------------------|
| Train	| 287113 |
| Validation | 13368 |
| Test	| 11490 |

## Other Information

This dataset contains 3 versions. Version 1.0.0 contains questions along with the articles, intended for question answering. Versions 2.0.0 and 3.0.0 are structured to support summarization rather than question answering. Version 2.0.0 contains anonymized data where named entities were replaced with unique identifiers. Version 3.0.0 contains non-anonymized data. 

We will be using Version 3.0.0 as it aligns best with our task of providing highlights of news articles.

| Column | Mean Token Count |
|--------|------------------|
| article | 781 |
| highlights | 56 |

Table: This table specifies the average token count for the `article` and `highlights` columns in the dataset.

# Data dictionary

| Column Name | Description | Type | Units | Example |
|-------------|-------------|------|-------|---------|
| article | A string containing the body of the news article | string | N/A | WASHINGTON (CNN) -- Vice President Dick Cheney will serve as acting president briefly Saturday while President Bush is anesthetized for a routine colonoscopy, White House spokesman Tony Snow said Friday. Bush is scheduled to have the medical procedure, expected to take about 2 1/2 hours, at the presidential retreat at Camp David, Maryland, Snow said. Bush's last colonoscopy was in June 2002, and no abnormalities were found, Snow said. The president's doctor had recommended a repeat procedure in about five years. The procedure will be supervised by Dr. Richard Tubb and conducted by a multidisciplinary team from the National Naval Medical Center in Bethesda, Maryland, Snow said. A colonoscopy is the most sensitive test for colon cancer, rectal cancer and polyps, small clumps of cells that can become cancerous, according to the Mayo Clinic. Small polyps may be removed during the procedure. Snow said that was the case when Bush had colonoscopies before becoming president. Snow himself is undergoing chemotherapy for cancer that began in his colon and spread to his liver. Snow told reporters he had a chemo session scheduled later Friday. Watch Snow talk about Bush's procedure and his own colon cancer Â» . "The president wants to encourage everybody to use surveillance," Snow said. The American Cancer Society recommends that people without high-risk factors or symptoms begin getting screened for signs of colorectal cancer at age 50. E-mail to a friend . |
| highlights | A string containing the highlight of the article as written by the article author | string | N/A | President Bush will have a routine colonoscopy Saturday . While he's anesthetized, his powers will be transferred to the vice president . Bush had last colonoscopy in 2002, which found no problems . |
| id | A string containing the hexadecimal formatted SHA1 hash of the url where the story was retrieved from | string | N/A | 35f0e33de7923036a97ac245d899f990bda5e242 |

Table: This table specifies which columns are in the dataset, along with their description, type, units, and examples.

# Steps to create the chatbot

To fine-tune our Chatbot, we use the [Unsloth](https://unsloth.ai/) library. This allows for simple and quick fine-tuning of a LLM. Specifically, we fine-tune Qwen2.5-7B using this [Python notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2.5_(7B)-Alpaca.ipynb) as guidance. This Python notebook is provided by Unsloth on their [PyPI page](https://pypi.org/project/unsloth/).

Here are the steps we took to fine-tune our model. Note: since the fine-tuning process took hours, we ran this code on a separate Python notebook using larget GPUs, and then copied the code into this Quarto file. As a result, the code isn't executable in this file.

## Install Packages

```python
%%capture
import os
if "COLAB_" not in "".join(os.environ.keys()):
    !pip install unsloth
else:
    # Do this only in Colab notebooks! Otherwise use pip install unsloth
    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo
    !pip install sentencepiece protobuf datasets huggingface_hub hf_transfer
    !pip install --no-deps unsloth
```

Note you will also need PyTorch installed. We don't include detailed instructions on installing PyTorch because we ran this code on Google Colab, and Google Colab's runtime comes with PyTorch pre-installed.

## Retrieve the pretrained model and tokeniser

```python
from unsloth import FastLanguageModel
import torch
max_seq_length = 2048
dtype = None
load_in_4bit = True

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "unsloth/Qwen2.5-7B",
    max_seq_length = max_seq_length,
    dtype = dtype,
    load_in_4bit = load_in_4bit
)
```

## Add LoRA adapters

This will allow us to only fine-tune 1% to 10% of all paramaters. This improves the speed of the training process since not all of the paramaters are fine-tuned.

```python
model = FastLanguageModel.get_peft_model(
    model,
    r = 16,
    target_modules = ["q_proj", "k_proj", "v_proj", "o_proj",
                      "gate_proj", "up_proj", "down_proj",],
    lora_alpha = 16,
    lora_dropout = 0,
    bias = "none",
    use_gradient_checkpointing = "unsloth",
    random_state = 3407,
    use_rslora = False,
    loftq_config = None,
)
```

## Training Data Preparation

Before we fine-tune our model, we have to prepare the data. 

First we load the `abisee/cnn_dailymail` dataset from Hugging Face and retrieve 1% of the train split. Since the train split has over 300,000 rows, we noticed that it was significantly more GPU intensive and time consuming to fine-tune a model. Thus, we take 1% of the train split to increase the feasbility of the fine-tuning process.

Once we load the data, we format it in the Alpaca Prompt format. The Alpaca Prompt format defines an Instruction, Input, and Response. The Input and Output change based on the data (different articles (inputs) have different highlights (outputs)). However, the Instruction is constant, and is set to: "Given the news article, determine the key highlights."

```python
alpaca_prompt = """Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.

### Instruction:
{}

### Input:
{}

### Response:
{}"""

EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN
def formatting_prompts_func(examples):
    texts = []
    for i in range(len(examples["article"])):
        instruction = "Given the news article, determine the key highlights."
        input_text = examples["article"][i]
        output_text = examples["highlights"][i]
        # Must add EOS_TOKEN, otherwise your generation will go on forever!
        text = alpaca_prompt.format(instruction, input_text, output_text) + EOS_TOKEN
        texts.append(text)
    return { "text" : texts, }
pass

from datasets import load_dataset
dataset = load_dataset("abisee/cnn_dailymail", name = "3.0.0", split = "train[:1%]")
dataset = dataset.map(formatting_prompts_func, batched = True,)
```

## Train the model

Now that we have the data prepared, we are ready to fine-tune the model. To do so, we will use [Hugging Face TRL's SFT Trainer](https://huggingface.co/docs/trl/sft_trainer). 

We set the `num_train_epochs = 1` to enable a full training run for the model.

```python
from trl import SFTTrainer
from transformers import TrainingArguments
from unsloth import is_bfloat16_supported

trainer = SFTTrainer(
    model = model,
    tokenizer = tokenizer,
    train_dataset = dataset,
    dataset_text_field = "text",
    max_seq_length = max_seq_length,
    dataset_num_proc = 2,
    packing = False,
    args = TrainingArguments(
        per_device_train_batch_size = 2,
        gradient_accumulation_steps = 4,
        warmup_steps = 5,
        num_train_epochs = 1,
        learning_rate = 2e-4,
        fp16 = not is_bfloat16_supported(),
        bf16 = is_bfloat16_supported(),
        logging_steps = 1,
        optim = "adamw_8bit",
        weight_decay = 0.01,
        lr_scheduler_type = "linear",
        seed = 3407,
        output_dir = "outputs",
        report_to = "none", # Use this for WandB etc
    ),
)
```

Now that we have the `SFTTrainer` configured, we can run the trainer, which will run the fine-tuning process. 

```python
trainer_stats = trainer.train()
```

## Saving the fine-tuned model

The fine-tuned model and the tokenizer can be saved locally by running the following code. It will save the fine-tuned model and tokenizer to a directory called `lora_model`.

```python
model.save_pretrained("lora_model")
tokenizer.save_pretrained("lora_model")
```

# Inference

Inference is the process of using a trained model to generate predictions or make decisions based on new, previously unseen data. We can use the model we just fine-tuned to generate highlights about new articles.

## Load the fine-tuned model

As mentioned before, we saved the fine-tuned model and tokenizer to a local directory called `lora_model`. We will load our fine-tuned model from that directory.

```python
from unsloth import FastLanguageModel
import torch

max_seq_length = 2048
dtype = None
load_in_4bit = True

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "lora_model",
    max_seq_length = max_seq_length,
    dtype = dtype,
    load_in_4bit = load_in_4bit,
)
FastLanguageModel.for_inference(model)
```

## Load Testing Data

We will load 1% of the test split, which we can use for inference.

```python
from datasets import load_dataset
test_dataset = load_dataset("abisee/cnn_dailymail", name = "3.0.0", split = "test[:1%]")
```

## Add data formatter

To ensure that the pre-trained model behaves as predicted, we want to ensure that the text we pass it matches the same format we used for training. To do this, we will use the same Alpaca Prompt template. However, we will set the `output_text` to be `""` since during inference, the output is not defined.

```python
alpaca_prompt = """Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.

### Instruction:
{}

### Input:
{}

### Response:
{}"""

def formatting_prompts_func(input_text):
    instruction = "Given the news article, determine the key highlights."
    output_text = ""
    text = alpaca_prompt.format(instruction, input_text, output_text)
    return text
```

## Create a function to run inference

Here, we create a `get_highlights` function, that takes in an article, formats it using the `formatting_prompts_func`, and then passes the formatted prompt to the fine-tuned model to get a response. The response is then returned.

```python
def get_highlights(article):
  inputs = tokenizer(
  [
      formatting_prompts_func(article)
  ], return_tensors = "pt").to("cuda")

  outputs = model.generate(**inputs, max_new_tokens = 128)
  decoded_output = tokenizer.batch_decode(outputs)
  return decoded_output[0].split('### Response:')[-1]
```

This function can be tested by running the following code.

```python
print(get_highlights(test_dataset[0]['article']))
```

## Gradio Interface

Install Gradio using the following code.

```python
!pip install gradio
```

To make the model easy-to-use (and more visually appealing), we can create a Gradio interface. Note, this Gradio interface will call our previously defined function `get_highlights`.

```python
import gradio as gr

interface = gr.Interface(
    fn=get_highlights,
    inputs="text",
    outputs="text",
    title="Fine-tuned Chatbot for news articles",
    description="Pass any news article and this ChatBot will return its key highlights. Powered by Qwen2.5-7B.",
)
```

Launch the Gradio interface by running the following code.

```python
interface.launch()
```

# Conclusion

In this Milestone, we covered how to use Unsloth to fine-tune Qwen2.5-7B on news articles and their highlights. We covered how this fine-tuned model can be saved locally. We also covered how to load the fine-tuned model and use it for inference. Finally, we created a Gradio iterface that makes the fine-tuned model easy-to-use.

# Appendix

The following contains the Python code to retrieve information about the dataset.

```{python}
dataset = "abisee/cnn_dailymail"
```

## Check if dataset is valid

```{python}
import requests
API_URL = f"https://datasets-server.huggingface.co/is-valid?dataset={dataset}"
def query():
    response = requests.get(API_URL)
    return response.json()
data = query()
print(data)
```

## Check if dataset has configurations and splits

Rotten tomatoes has a train and test split

```{python}
import requests
API_URL = f"https://datasets-server.huggingface.co/splits?dataset={dataset}"
def query():
    response = requests.get(API_URL)
    return response.json()
data = query()
print(data)
```

## Preview the dataset

Gives first rows

```{python}
import requests
API_URL = f"https://datasets-server.huggingface.co/first-rows?dataset={dataset}&config=3.0.0&split=train"
def query():
    response = requests.get(API_URL)
    return response.json()
data = query()
print(data)
```

## Download slices of dataset

```{python}
import requests
API_URL = f"https://datasets-server.huggingface.co/rows?dataset={dataset}&config=3.0.0&split=train&offset=150&length=10"
def query():
    response = requests.get(API_URL)
    return response.json()
data = query()
print(data)
```

## Access Parquet files
Haystack does this automatically
Parquet is a way of storing the data. Data starts off in different formats (json, csv, database format, etc.). For storage efficience and uniform interface, Huggingface converts it into Parquet format. And gives it back to me in whatever format I want.

```{python}
import requests
API_URL = f"https://datasets-server.huggingface.co/parquet?dataset={dataset}"
def query():
    response = requests.get(API_URL)
    return response.json()
data = query()
print(data)
```

## Get the size of the dataset

```{python}
import requests
API_URL = f"https://datasets-server.huggingface.co/size?dataset={dataset}"
def query():
    response = requests.get(API_URL)
    return response.json()
data = query()
print(data)
```
