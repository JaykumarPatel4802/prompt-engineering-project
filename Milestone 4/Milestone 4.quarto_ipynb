{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Milestone 4\"\n",
        "subtitle: \"Chatbot Evaluation\"\n",
        "date: today\n",
        "author: Jaykumar Patel, Laith Altarabishi, Harold Zhong\n",
        "format:\n",
        "  html:\n",
        "    toc: true\n",
        "    embed-resources: true\n",
        "mainfont: TeX Gyre Schola\n",
        "monofont: JetBrainsMono Nerd Font\n",
        "mathfont: TeX Gyre Schola Math Regular\n",
        "jupyter: python3\n",
        "---\n",
        "\n",
        "\n",
        "# Intro\n",
        "\n",
        "This documents the evaluation of our chatbot.\n",
        "\n",
        "Note: You will also need PyTorch and CUDA installed. We don't include detailed instructions on installing PyTorch because we ran this fine-tuning code using [Thunder Compute's A100 GPU](https://www.thundercompute.com/), and we didn't have to manually install PyTorch (it was already pre-installed).\n",
        "\n",
        "# Install Packages\n",
        "\n",
        "```python\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n",
        "    !pip install --no-deps unsloth\n",
        "\n",
        "# for Rouge\n",
        "! pip install evaluate\n",
        "! pip install rouge_score\n",
        "\n",
        "# for BertScore\n",
        "! pip install transformers\n",
        "! pip install bert-score\n",
        "```\n",
        "\n",
        "# Load Original Base Model\n",
        "\n",
        "We load the original base model `unsloth/unsloth/Qwen2.5-7B`.\n",
        "\n",
        "```python\n",
        "from unsloth import FastLanguageModel\n",
        "\n",
        "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "```\n",
        "\n",
        "```python\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Qwen2.5-7B\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        ")\n",
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "```\n",
        "\n",
        "# Load Fine-Tuned Model\n",
        "\n",
        "We saved the fine-tuned model and tokenizer to a local directory called `lora_model`. We load our fine-tuned model from that directory.\n",
        "\n",
        "```python\n",
        "from unsloth import FastLanguageModel\n",
        "\n",
        "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "```\n",
        "\n",
        "```python\n",
        "fine_tuned_model, fine_tuned_tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        ")\n",
        "FastLanguageModel.for_inference(fine_tuned_model) # Enable native 2x faster inference\n",
        "```\n",
        "\n",
        "# Testing Data Preparation\n",
        "\n",
        "We load the test data from `abisee/cnn_dailymail`. We use 1% of this data to make the evaluation process more feasible.\n",
        "\n",
        "```python\n",
        "from datasets import load_dataset\n",
        "test_dataset = load_dataset(\"abisee/cnn_dailymail\", name = \"3.0.0\", split = \"test[:1%]\")\n",
        "```\n",
        "\n",
        "```python\n",
        "print(len(test_dataset))\n",
        "print(test_dataset[0])\n",
        "```\n",
        "\n",
        "Console Output:\n",
        "\n",
        "```\n",
        "115\n",
        "{'article': '(CNN)The Palestinian Authority officially became the 123rd member of the International Criminal Court on Wednesday, a step that gives the court jurisdiction over alleged crimes in Palestinian territories. The formal accession was marked with a ceremony at The Hague, in the Netherlands, where the court is based. The Palestinians signed the ICC\\'s founding Rome Statute in January, when they also accepted its jurisdiction over alleged crimes committed \"in the occupied Palestinian territory, including East Jerusalem, since June 13, 2014.\" Later that month, the ICC opened a preliminary examination into the situation in Palestinian territories, paving the way for possible war crimes investigations against Israelis. As members of the court, Palestinians may be subject to counter-charges as well. Israel and the United States, neither of which is an ICC member, opposed the Palestinians\\' efforts to join the body. But Palestinian Foreign Minister Riad al-Malki, speaking at Wednesday\\'s ceremony, said it was a move toward greater justice. \"As Palestine formally becomes a State Party to the Rome Statute today, the world is also a step closer to ending a long era of impunity and injustice,\" he said, according to an ICC news release. \"Indeed, today brings us closer to our shared goals of justice and peace.\" Judge Kuniko Ozaki, a vice president of the ICC, said acceding to the treaty was just the first step for the Palestinians. \"As the Rome Statute today enters into force for the State of Palestine, Palestine acquires all the rights as well as responsibilities that come with being a State Party to the Statute. These are substantive commitments, which cannot be taken lightly,\" she said. Rights group Human Rights Watch welcomed the development. \"Governments seeking to penalize Palestine for joining the ICC should immediately end their pressure, and countries that support universal acceptance of the court\\'s treaty should speak out to welcome its membership,\" said Balkees Jarrah, international justice counsel for the group. \"What\\'s objectionable is the attempts to undermine international justice, not Palestine\\'s decision to join a treaty to which over 100 countries around the world are members.\" In January, when the preliminary ICC examination was opened, Israeli Prime Minister Benjamin Netanyahu described it as an outrage, saying the court was overstepping its boundaries. The United States also said it \"strongly\" disagreed with the court\\'s decision. \"As we have said repeatedly, we do not believe that Palestine is a state and therefore we do not believe that it is eligible to join the ICC,\" the State Department said in a statement. It urged the warring sides to resolve their differences through direct negotiations. \"We will continue to oppose actions against Israel at the ICC as counterproductive to the cause of peace,\" it said. But the ICC begs to differ with the definition of a state for its purposes and refers to the territories as \"Palestine.\" While a preliminary examination is not a formal investigation, it allows the court to review evidence and determine whether to investigate suspects on both sides. Prosecutor Fatou Bensouda said her office would \"conduct its analysis in full independence and impartiality.\" The war between Israel and Hamas militants in Gaza last summer left more than 2,000 people dead. The inquiry will include alleged war crimes committed since June. The International Criminal Court was set up in 2002 to prosecute genocide, crimes against humanity and war crimes. CNN\\'s Vasco Cotovio, Kareem Khadder and Faith Karimi contributed to this report.', 'highlights': 'Membership gives the ICC jurisdiction over alleged crimes committed in Palestinian territories since last June .\\nIsrael and the United States opposed the move, which could open the door to war crimes investigations against Israelis .', 'id': 'f001ec5c4704938247d27a44948eebb37ae98d01'}\n",
        "```\n",
        "\n",
        "# Add formatter\n",
        "\n",
        "To ensure that the fine-tuned model behaves as predicted, we want to ensure that the text we pass it matches the same format we used for training. To do this, we will use the same Alpaca Prompt template. However, we will set the `output_text` to be `\"\"` since during inference, the output is not defined.\n",
        "\n",
        "```python\n",
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "def formatting_prompts_func(input_text):\n",
        "    instruction = \"Given the news article, determine the key highlights.\"\n",
        "    output_text = \"\"\n",
        "    text = alpaca_prompt.format(instruction, input_text, output_text)\n",
        "    return text\n",
        "```\n",
        "\n",
        "```python\n",
        "# Testing the formatter\n",
        "print(formatting_prompts_func(test_dataset[0]['article']))\n",
        "```\n",
        "\n",
        "Console Output:\n",
        "\n",
        "```\n",
        "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "Given the news article, determine the key highlights.\n",
        "\n",
        "### Input:\n",
        "(CNN)The Palestinian Authority officially became the 123rd member of the International Criminal Court on Wednesday, a step that gives the court jurisdiction over alleged crimes in Palestinian territories. The formal accession was marked with a ceremony at The Hague, in the Netherlands, where the court is based. The Palestinians signed the ICC's founding Rome Statute in January, when they also accepted its jurisdiction over alleged crimes committed \"in the occupied Palestinian territory, including East Jerusalem, since June 13, 2014.\" Later that month, the ICC opened a preliminary examination into the situation in Palestinian territories, paving the way for possible war crimes investigations against Israelis. As members of the court, Palestinians may be subject to counter-charges as well. Israel and the United States, neither of which is an ICC member, opposed the Palestinians' efforts to join the body. But Palestinian Foreign Minister Riad al-Malki, speaking at Wednesday's ceremony, said it was a move toward greater justice. \"As Palestine formally becomes a State Party to the Rome Statute today, the world is also a step closer to ending a long era of impunity and injustice,\" he said, according to an ICC news release. \"Indeed, today brings us closer to our shared goals of justice and peace.\" Judge Kuniko Ozaki, a vice president of the ICC, said acceding to the treaty was just the first step for the Palestinians. \"As the Rome Statute today enters into force for the State of Palestine, Palestine acquires all the rights as well as responsibilities that come with being a State Party to the Statute. These are substantive commitments, which cannot be taken lightly,\" she said. Rights group Human Rights Watch welcomed the development. \"Governments seeking to penalize Palestine for joining the ICC should immediately end their pressure, and countries that support universal acceptance of the court's treaty should speak out to welcome its membership,\" said Balkees Jarrah, international justice counsel for the group. \"What's objectionable is the attempts to undermine international justice, not Palestine's decision to join a treaty to which over 100 countries around the world are members.\" In January, when the preliminary ICC examination was opened, Israeli Prime Minister Benjamin Netanyahu described it as an outrage, saying the court was overstepping its boundaries. The United States also said it \"strongly\" disagreed with the court's decision. \"As we have said repeatedly, we do not believe that Palestine is a state and therefore we do not believe that it is eligible to join the ICC,\" the State Department said in a statement. It urged the warring sides to resolve their differences through direct negotiations. \"We will continue to oppose actions against Israel at the ICC as counterproductive to the cause of peace,\" it said. But the ICC begs to differ with the definition of a state for its purposes and refers to the territories as \"Palestine.\" While a preliminary examination is not a formal investigation, it allows the court to review evidence and determine whether to investigate suspects on both sides. Prosecutor Fatou Bensouda said her office would \"conduct its analysis in full independence and impartiality.\" The war between Israel and Hamas militants in Gaza last summer left more than 2,000 people dead. The inquiry will include alleged war crimes committed since June. The International Criminal Court was set up in 2002 to prosecute genocide, crimes against humanity and war crimes. CNN's Vasco Cotovio, Kareem Khadder and Faith Karimi contributed to this report.\n",
        "\n",
        "### Response:\n",
        "```\n",
        "\n",
        "# Run Inference\n",
        "\n",
        "Inference is the process of using a trained model to generate predictions or make decisions based on new, previously unseen data.\n",
        "\n",
        "## Create `get_highlights` function that runs inference\n",
        "\n",
        "Here, we create a `get_highlights` function, that takes in an article, formats it using the `formatting_prompts_func`, and then passes the formatted prompt to the fine-tuned model to get a response. The response is then returned.\n",
        "\n",
        "The `get_highlights` function takes a parameter `fine_tuned`. When `fine_tuned` is set to `True`, then the function uses the fine-tuned model to make the inference (generate highlights). When `fine_tuned` is set to `False`, then the function uses the original base model to make the inference (generate highlights).\n",
        "\n",
        "The `get_highlights` function also calculates the time it takes to run the inference. This information is useful in inference time analysis.\n",
        "\n",
        "```python\n",
        "import time\n",
        "\n",
        "def get_highlights(article, fine_tuned = False):\n",
        "  start = time.time()\n",
        "  if fine_tuned:\n",
        "    inputs = fine_tuned_tokenizer(\n",
        "    [\n",
        "      formatting_prompts_func(article)\n",
        "    ], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "    outputs = fine_tuned_model.generate(**inputs, max_new_tokens = 128)\n",
        "    decoded_output = fine_tuned_tokenizer.batch_decode(outputs)\n",
        "\n",
        "    end = time.time()\n",
        "\n",
        "    return decoded_output[0].split('### Response:')[-1].replace(fine_tuned_tokenizer.eos_token, ''), end - start\n",
        "\n",
        "  inputs = tokenizer(\n",
        "  [\n",
        "    formatting_prompts_func(article)\n",
        "  ], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "  outputs = model.generate(**inputs, max_new_tokens = 128)\n",
        "  decoded_output = tokenizer.batch_decode(outputs)\n",
        "\n",
        "  end = time.time()\n",
        "\n",
        "  return decoded_output[0].split('### Response:')[-1], end - start\n",
        "```\n",
        "\n",
        "## Test `get_highlights` function\n",
        "\n",
        "Here we test the `get_highlights` function to ensure that it works correctly.\n",
        "\n",
        "### Test 1\n",
        "\n",
        "```python\n",
        "import random\n",
        "\n",
        "random_index = random.randint(0, len(test_dataset) - 1)\n",
        "print(f\"Testing index: {random_index}\")\n",
        "\n",
        "print(\"Article:\")\n",
        "print(test_dataset[random_index]['article'])\n",
        "print()\n",
        "print(\"Highlights\")\n",
        "print(test_dataset[random_index]['highlights'])\n",
        "print()\n",
        "\n",
        "print(\"Original Model Test Highlights\")\n",
        "original_model_test = get_highlights(test_dataset[random_index]['article'], fine_tuned=False)\n",
        "print(original_model_test[0])\n",
        "print(\"Original Model Test Time\")\n",
        "print(original_model_test[1])\n",
        "\n",
        "print()\n",
        "\n",
        "print(\"Fine-tuned Model Test Highlights\")\n",
        "fine_tuned_model_test = get_highlights(test_dataset[random_index]['article'], fine_tuned=True)\n",
        "print(fine_tuned_model_test[0])\n",
        "print(\"Fine-tuned Model Test Time\")\n",
        "print(fine_tuned_model_test[1])\n",
        "```\n",
        "\n",
        "Console Output:\n",
        "\n",
        "```\n",
        "Testing index: 0\n",
        "Article:\n",
        "(CNN)The Palestinian Authority officially became the 123rd member of the International Criminal Court on Wednesday, a step that gives the court jurisdiction over alleged crimes in Palestinian territories. The formal accession was marked with a ceremony at The Hague, in the Netherlands, where the court is based. The Palestinians signed the ICC's founding Rome Statute in January, when they also accepted its jurisdiction over alleged crimes committed \"in the occupied Palestinian territory, including East Jerusalem, since June 13, 2014.\" Later that month, the ICC opened a preliminary examination into the situation in Palestinian territories, paving the way for possible war crimes investigations against Israelis. As members of the court, Palestinians may be subject to counter-charges as well. Israel and the United States, neither of which is an ICC member, opposed the Palestinians' efforts to join the body. But Palestinian Foreign Minister Riad al-Malki, speaking at Wednesday's ceremony, said it was a move toward greater justice. \"As Palestine formally becomes a State Party to the Rome Statute today, the world is also a step closer to ending a long era of impunity and injustice,\" he said, according to an ICC news release. \"Indeed, today brings us closer to our shared goals of justice and peace.\" Judge Kuniko Ozaki, a vice president of the ICC, said acceding to the treaty was just the first step for the Palestinians. \"As the Rome Statute today enters into force for the State of Palestine, Palestine acquires all the rights as well as responsibilities that come with being a State Party to the Statute. These are substantive commitments, which cannot be taken lightly,\" she said. Rights group Human Rights Watch welcomed the development. \"Governments seeking to penalize Palestine for joining the ICC should immediately end their pressure, and countries that support universal acceptance of the court's treaty should speak out to welcome its membership,\" said Balkees Jarrah, international justice counsel for the group. \"What's objectionable is the attempts to undermine international justice, not Palestine's decision to join a treaty to which over 100 countries around the world are members.\" In January, when the preliminary ICC examination was opened, Israeli Prime Minister Benjamin Netanyahu described it as an outrage, saying the court was overstepping its boundaries. The United States also said it \"strongly\" disagreed with the court's decision. \"As we have said repeatedly, we do not believe that Palestine is a state and therefore we do not believe that it is eligible to join the ICC,\" the State Department said in a statement. It urged the warring sides to resolve their differences through direct negotiations. \"We will continue to oppose actions against Israel at the ICC as counterproductive to the cause of peace,\" it said. But the ICC begs to differ with the definition of a state for its purposes and refers to the territories as \"Palestine.\" While a preliminary examination is not a formal investigation, it allows the court to review evidence and determine whether to investigate suspects on both sides. Prosecutor Fatou Bensouda said her office would \"conduct its analysis in full independence and impartiality.\" The war between Israel and Hamas militants in Gaza last summer left more than 2,000 people dead. The inquiry will include alleged war crimes committed since June. The International Criminal Court was set up in 2002 to prosecute genocide, crimes against humanity and war crimes. CNN's Vasco Cotovio, Kareem Khadder and Faith Karimi contributed to this report.\n",
        "\n",
        "Highlights\n",
        "Membership gives the ICC jurisdiction over alleged crimes committed in Palestinian territories since last June .\n",
        "Israel and the United States opposed the move, which could open the door to war crimes investigations against Israelis .\n",
        "\n",
        "Original Model Test Highlights\n",
        "\n",
        "The key highlights of the news article are:\n",
        "\n",
        "1. The Palestinian Authority officially became the 123rd member of the International Criminal Court (ICC) on Wednesday, giving the court jurisdiction over alleged crimes in Palestinian territories.\n",
        "2. The Palestinians signed the ICC's founding Rome Statute in January, accepting its jurisdiction over alleged crimes committed in the occupied Palestinian territory, including East Jerusalem, since June 13, 2. The ICC opened a preliminary examination into the situation in Palestinian territories, paving the way for possible war crimes investigations against Israelis.\n",
        " 3. As members of the court, Palestinians may be subject to counter-charges as\n",
        "Original Model Test Time\n",
        "10.787961959838867\n",
        "\n",
        "Fine-tuned Model Test Highlights\n",
        "\n",
        "Palestinian Authority officially becomes 123rd member of the International Criminal Court .\n",
        "Palestinians may be subject to counter-charges as well .\n",
        "Israel and the United States, neither of which is an ICC member, opposed the move .\n",
        "Fine-tuned Model Test Time\n",
        "5.0052711963653564\n",
        "```\n",
        "\n",
        "### Test 2\n",
        "\n",
        "Here we test the `get_highlights` function again to ensure that it works correctly.\n",
        "\n",
        "```python\n",
        "random_index_2 = random.randint(0, len(test_dataset) - 1)\n",
        "print(f\"Testing index: {random_index_2}\")\n",
        "\n",
        "print(\"Article:\")\n",
        "print(test_dataset[random_index_2]['article'])\n",
        "print()\n",
        "print(\"Highlights\")\n",
        "print(test_dataset[random_index_2]['highlights'])\n",
        "print()\n",
        "\n",
        "print(\"Original Model Test Highlights\")\n",
        "original_model_test_2 = get_highlights(test_dataset[random_index_2]['article'], fine_tuned=False)\n",
        "print(original_model_test_2[0])\n",
        "print(\"Original Model Test Time\")\n",
        "print(original_model_test_2[1])\n",
        "\n",
        "print()\n",
        "\n",
        "print(\"Fine-tuned Model Test Highlights\")\n",
        "fine_tuned_model_test_2 = get_highlights(test_dataset[random_index_2]['article'], fine_tuned=True)\n",
        "print(fine_tuned_model_test_2[0])\n",
        "print(\"Fine-tuned Model Test Time\")\n",
        "print(fine_tuned_model_test_2[1])\n",
        "```\n",
        "\n",
        "Console Output:\n",
        "\n",
        "```\n",
        "Testing index: 81\n",
        "Article:\n",
        "(CNN)One hundred and forty-seven victims. Many more families affected. Even more broken hopes and dreams. As Kenyans mourned those killed last week in one of the deadliest terrorist attacks in the nation, citizens used social media to share the victims' stories, hopes and dreams. Using the hashtag #147notjustanumber -- a reference to the number of people, mostly students, killed at Garissa University College on Thursday -- Kenyans tweeted pictures of the victims in happier times. Kenyan authorities have not released a list of the victims. The posts provided heart-wrenching details on the victims, including one about an elderly man whose dreams died with his son. He had reportedly taken a loan to educate him at the university, where he was killed by Al-Shabaab terrorists. The attack in Kenya killed 142 students, three security officers and two university security personnel, and was the nation's deadliest since the bombing of the U.S. Embassy in 1998. Kenyan churches mourned the dead during Easter services Sunday as armed guards protected the congregations. In emotional services nationwide, churchgoers wept as they paid tribute to the victims of the massacre. The gunmen who attacked the university in the predawn hours separated Muslims from Christians and killed the latter. The extremist group has also killed Muslims in recent attacks. The Interior Ministry has identified one of the attackers killed by security forces as the son of a government official. The father of suspect Abdirahim Abdullahi is a chief in Mandera and had reported his son missing, officials said. The Islamist extremist group is based in Somalia, but it hasn't confined its terrorism to the nation that shares a border with Kenya. In 2013, militants attacked Nairobi's upscale Westgate Mall, killing nearly 70 people.\n",
        "\n",
        "Highlights\n",
        "Kenyans use hashtag #147notjustanumber to honor victims of Kenya university attack .\n",
        "The attack killed 142 students, three security officers and two university security personnel .\n",
        "\n",
        "Original Model Test Highlights\n",
        "\n",
        "The key highlights of the news article are:\n",
        "\n",
        "1. **Massive Attack**: A terrorist attack occurred at Garissa University College, resulting in 147 deaths, making it one of the deadliest in Kenya.\n",
        "2. **Victims' Stories**: Kenyans shared the victims' stories, hopes, and dreams on social media using the hashtag #147notjustanumber.\n",
        "3. **Attack Details**: The attackers separated Muslims from Christians and targeted Christians, while the group has also killed Muslims in recent attacks.\n",
        "4. **Suspect Information**: One of the attackers was identified as the son of a government official, whose\n",
        "Original Model Test Time\n",
        "9.827091217041016\n",
        "\n",
        "Fine-tuned Model Test Highlights\n",
        "\n",
        "Kenyans share stories of victims on social media using #147notjustanumber .\n",
        "The attack in Kenya killed 142 students, three security officers and two university personnel .\n",
        "The gunmen who attacked the university in the predawn hours separated Muslims from Christians .\n",
        "Fine-tuned Model Test Time\n",
        "6.604422569274902\n",
        "```\n",
        "\n",
        "## Run Inference on Testing Data\n",
        "\n",
        "```python\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "\n",
        "final_data = []\n",
        "for test_item in tqdm(test_dataset):\n",
        "  ground_truth_highlights = test_item['highlights']\n",
        "  original_model_highlights, original_model_time = get_highlights(test_item['article'], fine_tuned = False)\n",
        "  fine_tuned_model_highlights, fine_tuned_model_time = get_highlights(test_item['article'], fine_tuned = True)\n",
        "\n",
        "  final_data.append({\n",
        "    \"article\": test_item['article'],\n",
        "    \"ground_truth_all_highlights\": ground_truth_highlights,\n",
        "    \"original_model_all_data\": (original_model_highlights, original_model_time),\n",
        "    \"fine_tuned_model_all_data\": (fine_tuned_model_highlights, fine_tuned_model_time)\n",
        "  })\n",
        "\n",
        "ground_truth_all_highlights = [data['ground_truth_all_highlights'] for data in final_data]\n",
        "\n",
        "original_model_all_highlights = [data['original_model_all_data'][0] for data in final_data]\n",
        "original_model_all_times = [data['original_model_all_data'][1] for data in final_data]\n",
        "\n",
        "fine_tuned_model_all_highlights = [data['fine_tuned_model_all_data'][0] for data in final_data]\n",
        "fine_tuned_model_all_times = [data['fine_tuned_model_all_data'][1] for data in final_data]\n",
        "\n",
        "# save data to file for backup\n",
        "with open(\"final_data.json\", \"w\") as f:\n",
        "  json.dump(final_data, f)\n",
        "```\n",
        "\n",
        "# Evaluate\n",
        "\n",
        "## Rogue\n",
        "\n",
        "Here, we use the `evaluate` library to calculate the Rogue Score for the highlights generated by the fine-tuned model and the original base model. We use the highlights in the original dataset as reference.\n",
        "\n",
        "We followed this [tutorial](https://medium.com/@eren9677/text-summarization-387836c9e178) to write this code.\n",
        "\n",
        "```python\n",
        "import evaluate\n",
        "rouge = evaluate.load('rouge')\n",
        "\n",
        "original_model_results = rouge.compute(\n",
        "    predictions = original_model_all_highlights,\n",
        "    references = ground_truth_all_highlights\n",
        ")\n",
        "\n",
        "fine_tuned_model_results = rouge.compute(\n",
        "    predictions = fine_tuned_model_all_highlights,\n",
        "    references = ground_truth_all_highlights\n",
        ")\n",
        "\n",
        "print(original_model_results)\n",
        "print(fine_tuned_model_results)\n",
        "```\n",
        "\n",
        "Console Output:\n",
        "\n",
        "```\n",
        "{'rouge1': np.float64(0.25593852325683963), 'rouge2': np.float64(0.09879602742572864), 'rougeL': np.float64(0.1845369489659137), 'rougeLsum': np.float64(0.23482225329274253)}\n",
        "{'rouge1': np.float64(0.3202412916174494), 'rouge2': np.float64(0.1291838902741764), 'rougeL': np.float64(0.23471503036165922), 'rougeLsum': np.float64(0.29682001157527327)}\n",
        "```\n",
        "\n",
        "Table View:\n",
        "\n",
        "| Model | rouge1 | rouge2 | rougeL | rougeLsum |\n",
        "|-------|--------|--------|--------|-----------|\n",
        "| Original Base Model | 0.25593852325683963 | 0.09879602742572864 | 0.1845369489659137 | 0.23482225329274253 |\n",
        "| Fine-tuned Model | 0.3202412916174494 | 0.1291838902741764 | 0.23471503036165922 | 0.29682001157527327 |\n",
        "\n",
        "### Interpretation\n",
        "\n",
        "ROUGE (Recall-Oriented Understudy for Gisting Evaluation) is a commonly used metric for evaluating the quality of generated summaries by comparing them to reference summaries. In our case, we use it to evaluate the highlights generated by the original base model and the fine-tuned model, using the highlights provided in the dataset as reference.\n",
        "\n",
        "The ROUGE-1 and ROUGE-2 scores represent unigram and bigram overlap, respectively, while ROUGE-L and ROUGE-Lsum evaluate the longest common subsequence between the generated and reference texts—giving insight into fluency and content preservation.\n",
        "\n",
        "From the results, it is evident that the fine-tuned model outperforms the original base model across all ROUGE metric indicating a better word-level content capture and phrase-level coherence.\n",
        "\n",
        "## BertScore\n",
        "\n",
        "Here, we use the `bert_score` library to calculate the precision, recall, and F1 for the highlights generated by the fine-tuned model and the original base model. We use the highlights in the original dataset as reference.\n",
        "\n",
        "We followed this [tutorial](https://haticeozbolat17.medium.com/text-summarization-how-to-calculate-bertscore-771a51022964) to write this code.\n",
        "\n",
        "```python\n",
        "from transformers import BertTokenizer, BertForMaskedLM, BertModel\n",
        "from bert_score import BERTScorer\n",
        "\n",
        "# BERTScore calculation\n",
        "scorer = BERTScorer(model_type='bert-base-uncased')\n",
        "P_original_model, R_original_model, F1_original_model = scorer.score(original_model_all_highlights, ground_truth_all_highlights)\n",
        "P_fine_tuned_model, R_fine_tuned_model, F1_fine_tuned_model = scorer.score(fine_tuned_model_all_highlights, ground_truth_all_highlights)\n",
        "\n",
        "print(f\"BERTScore Precision for original model: {P_original_model.mean():.4f}, Recall: {R_original_model.mean():.4f}, F1: {F1_original_model.mean():.4f}\")\n",
        "print(f\"BERTScore Precision for fine-tuned model: {P_fine_tuned_model.mean():.4f}, Recall: {R_fine_tuned_model.mean():.4f}, F1: {F1_fine_tuned_model.mean():.4f}\")\n",
        "```\n",
        "\n",
        "Console Output:\n",
        "\n",
        "```\n",
        "BERTScore Precision for original model: 0.4919, Recall: 0.6337, F1: 0.5528\n",
        "BERTScore Precision for fine-tuned model: 0.5750, Recall: 0.6091, F1: 0.5904\n",
        "```\n",
        "\n",
        "Table View:\n",
        "\n",
        "| Model | Precision | Recall | F1 |\n",
        "|-------|-----------|--------|----|\n",
        "| Original Base Model | 0.4919 | 0.6337 | 0.5528 |\n",
        "| Fine-tuned Model | 0.5750 | 0.6091 | 0.5904 |\n",
        "\n",
        "### Interpretation\n",
        "\n",
        "To further evaluate model performance, we report precision, recall, and F1 scores. These metrics offer complementary views of how well the generated highlights align with the reference highlights.\n",
        "\n",
        "- Precision measures the proportion of generated highlights that are relevant (i.e., present in the reference).\n",
        "\n",
        "- Recall measures the proportion of reference highlights that are correctly captured by the model.\n",
        "\n",
        "- F1 Score is the harmonic mean of Precision and Recall, balancing both aspects.\n",
        "\n",
        "From the table, we observe that the fine-tuned model achieves higher Precision (0.5750 vs. 0.4919) and F1 score (0.5904 vs. 0.5528), indicating that it generates more relevant and balanced highlights overall. Interestingly, while recall slightly decreases (0.6091 vs. 0.6337), this trade-off is acceptable as the improvement in precision leads to a more meaningful and concise summary generation.\n",
        "\n",
        "These results reinforce the benefit of fine-tuning, showing that it leads to more accurate and focused highlight generation without significantly sacrificing recall.\n",
        "\n",
        "## Inference Time Analysis\n",
        "\n",
        "Next we compare the inference time between the fine-tuned model and the original base model. Inference time is a critical metric for evaluating the practical usability of a model, especially in real-time or resource-constrained environments.\n",
        "\n",
        "### Basic Stats\n",
        "\n",
        "```python\n",
        "# Basic Stats\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def print_inference_stats(times, model_name):\n",
        "    times = np.array(times)\n",
        "    print(f\"--- {model_name} ---\")\n",
        "    print(f\"Mean:    {np.mean(times):.4f} ms\")\n",
        "    print(f\"Median:  {np.median(times):.4f} ms\")\n",
        "    print(f\"Std Dev: {np.std(times):.4f} ms\")\n",
        "    print(f\"Min:     {np.min(times):.4f} ms\")\n",
        "    print(f\"Max:     {np.max(times):.4f} ms\")\n",
        "    print()\n",
        "\n",
        "print_inference_stats(original_model_all_times, \"Model A\")\n",
        "print_inference_stats(fine_tuned_model_all_times, \"Model B\")\n",
        "```\n",
        "\n",
        "Console Output (Model A is the original base model and Model B is the fine-tuned model):\n",
        "\n",
        "```\n",
        "--- Model A ---\n",
        "Mean:    11.4647 sec\n",
        "Median:  11.3639 sec\n",
        "Std Dev: 1.5827 sec\n",
        "Min:     8.9727 sec\n",
        "Max:     16.8651 sec\n",
        "\n",
        "--- Model B ---\n",
        "Mean:    7.5664 sec\n",
        "Median:  7.3395 sec\n",
        "Std Dev: 2.2447 sec\n",
        "Min:     3.5611 sec\n",
        "Max:     17.9274 sec\n",
        "```\n",
        "\n",
        "Table View:\n",
        "\n",
        "| Model | Mean | Median | Std Dev | Min | Max |\n",
        "|-------|------|--------|---------|-----|-----|\n",
        "| Original Base Model | 11.4647 sec | 11.3639 sec | 1.5827 sec | 8.9727 sec | 16.8651 sec |\n",
        "| Fine-tuned Model | 7.5664 sec | 7.3395 sec | 2.2447 sec | 3.5611 sec | 17.9274 sec |\n",
        "\n",
        "#### Interpretation\n",
        "\n",
        "From the statistics:\n",
        "\n",
        "- Original Base Model (Model A) has a mean inference time of 11.46 seconds, with relatively low variability (standard deviation of 1.58 sec). Its inference times are more consistent, with a narrower range between the minimum (8.97 sec) and maximum (16.87 sec).\n",
        "\n",
        "Fine-tuned Model (Model B) is significantly faster, with a mean inference time of 7.57 seconds, representing a ~34% reduction in average runtime. However, it shows greater variability (standard deviation of 2.24 sec), and a wider range, from as low as 3.56 sec to as high as 17.93 sec.\n",
        "\n",
        "Overall, while the fine-tuned model is faster on average, its inference time is less stable. Depending on the deployment scenario, this trade-off may be acceptable or even desirable, especially if lower average latency is prioritized over consistency.\n",
        "\n",
        "### Box Plot\n",
        "\n",
        "The box plot highlights key summary statistics such as the median, interquartile range, and potential outliers.\n",
        "\n",
        "```python\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Box Plot (for quick comparison of median, spread, outliers)\n",
        "plt.boxplot([original_model_all_times, fine_tuned_model_all_times], labels=[\"Original Model\", \"Fine-tuned Model\"])\n",
        "plt.ylabel(\"Inference Time (ms)\")  # or whatever unit\n",
        "plt.title(\"Comparison of Inference Times\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "![](Box Plot.png)\n",
        "\n",
        "#### Interpretation\n",
        "\n",
        "The box plot compares the distribution of inference times for the original base model and the fine-tuned model.\n",
        "\n",
        "- The fine-tuned model has a noticeably lower median inference time, indicating faster performance most of the time. However, it also shows greater variability, with a wider spread and more outliers, especially on the higher end.\n",
        "\n",
        "- In contrast, the original model has a higher and more stable median, with a tighter interquartile range and fewer extreme values.\n",
        "\n",
        "This visual reinforces the earlier statistical findings: while the fine-tuned model offers faster inference overall, it introduces more variability in runtime, which may be a consideration for real-time or latency-sensitive applications.\n",
        "\n",
        "\n",
        "### Violin Plot\n",
        "\n",
        "The violin plot adds a smoothed density estimate, providing a clearer picture of the distribution’s shape and variance.\n",
        "\n",
        "```python\n",
        "# Violin Plot (fancier version of boxplot + distribution)\n",
        "\n",
        "plt.violinplot([original_model_all_times, fine_tuned_model_all_times], showmeans=True)\n",
        "plt.xticks([1, 2], [\"Original Model\", \"Fine-tuned Model\"])\n",
        "plt.ylabel(\"Inference Time (ms)\")\n",
        "plt.title(\"Violin Plot of Inference Times\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "![](Violin Plot.png)\n",
        "\n",
        "#### Interpretation\n",
        "\n",
        "The violin plot provides a detailed view of the distribution shape for inference times of both models.\n",
        "\n",
        "- The fine-tuned model, while centered at a lower median (~7 seconds), displays a wider and more asymmetric distribution, with some extreme values stretching as high as 18 seconds. The density bulge around 6–8 seconds for the fine-tuned model suggests that most inferences are fast, but occasional spikes in inference time contribute to its higher variability.\n",
        "\n",
        "- The original base model shows a tight distribution centered around 11–12 seconds, with a smaller spread. This indicates consistent performance with few outliers and low variance.\n",
        "\n",
        "This visualization reinforces the trade-off: the fine-tuned model is typically faster but less predictable, whereas the original model offers more consistent but slower inference times.\n",
        "\n",
        "\n",
        "### Cumulative Distribution Function (CDF)\n",
        "\n",
        "The Cumulative Distribution Function (CDF) plot shows the proportion of inferences completed within a given time, helping visualize how quickly each model reaches different latency thresholds.\n",
        "\n",
        "```python\n",
        "# Cumulative Distribution Function (CDF) (good for seeing which model is faster X% of the time)\n",
        "\n",
        "# Sort the times\n",
        "original_model_all_times_sorted = np.sort(original_model_all_times)\n",
        "fine_tuned_model_all_times_sorted = np.sort(fine_tuned_model_all_times)\n",
        "\n",
        "# Compute cumulative probabilities\n",
        "cdf_original_times = np.arange(len(original_model_all_times_sorted)) / len(original_model_all_times_sorted)\n",
        "cdf_fine_tuned_times = np.arange(len(fine_tuned_model_all_times_sorted)) / len(fine_tuned_model_all_times_sorted)\n",
        "\n",
        "plt.plot(original_model_all_times_sorted, cdf_original_times, label=\"Original Model\")\n",
        "plt.plot(fine_tuned_model_all_times_sorted, cdf_fine_tuned_times, label=\"Fine-tuned Model\")\n",
        "plt.xlabel(\"Inference Time (ms)\")\n",
        "plt.ylabel(\"Cumulative Probability\")\n",
        "plt.title(\"CDF of Inference Times\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "![](CDF.png)\n",
        "\n",
        "#### Interpretation\n",
        "\n",
        "The CDF plot illustrates the proportion of inference runs completed within a given amount of time for both models.\n",
        "\n",
        "- The fine-tuned model consistently completes inferences faster: over 80% of its inferences finish before 9 seconds, which is approximately the fastest inference time achieved by the original model. This highlights a substantial latency improvement. However, the fine-tuned model also shows a longer tail, with a few outlier inferences taking up to 18 seconds. \n",
        "\n",
        "- In contrast, the original model has a more compact range, with all inferences completing between roughly 9 and 17 seconds.\n",
        "\n",
        "Overall, the fine-tuned model delivers faster results for the majority of inputs, reinforcing its advantage in latency-critical settings."
      ],
      "id": "4de606bb"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "C:\\Users\\patel\\AppData\\Roaming\\Python\\share\\jupyter\\kernels\\python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}